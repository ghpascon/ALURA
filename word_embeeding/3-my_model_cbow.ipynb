{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "from unidecode import unidecode\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import spacy\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : - %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Após polêmica, Marine Le Pen diz que abomina n...   \n",
      "1  Macron e Le Pen vão ao 2º turno na França, em ...   \n",
      "2  Apesar de larga vitória nas legislativas, Macr...   \n",
      "3  Governo antecipa balanço, e Alckmin anuncia qu...   \n",
      "4  Após queda em maio, a atividade econômica sobe...   \n",
      "\n",
      "                                                text        date   category  \\\n",
      "0  A candidata da direita nacionalista à Presidên...  2017-04-28      mundo   \n",
      "1  O centrista independente Emmanuel Macron e a d...  2017-04-23      mundo   \n",
      "2  As eleições legislativas deste domingo (19) na...  2017-06-19      mundo   \n",
      "3  O número de ocorrências de homicídios dolosos ...  2015-07-24  cotidiano   \n",
      "4  A economia cresceu 0,25% no segundo trimestre,...  2017-08-17    mercado   \n",
      "\n",
      "  subcategory                                               link  \n",
      "0         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
      "1         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
      "2         NaN  http://www1.folha.uol.com.br/mundo/2017/06/189...  \n",
      "3         NaN  http://www1.folha.uol.com.br/cotidiano/2015/07...  \n",
      "4         NaN  http://www1.folha.uol.com.br/mercado/2017/08/1...  \n",
      "                                               title  \\\n",
      "0                                     Grandes irmãos   \n",
      "1  Haddad congela orçamento e suspende emendas de...   \n",
      "2  Proposta de reforma da Fifa tem a divulgação d...   \n",
      "3  Mercado incipiente, internet das coisas conect...   \n",
      "4  Mortes: Psicanalista, estudou o autismo em cri...   \n",
      "\n",
      "                                                text        date   category  \\\n",
      "0  RIO DE JANEIRO - O Brasil, cada vez menos famí...  2017-03-06    colunas   \n",
      "1  O prefeito de São Paulo, Fernando Haddad (PT),...  2016-08-10    colunas   \n",
      "2  A Fifa divulgou, nesta quinta (10), um relatór...  2015-10-09    esporte   \n",
      "3  Bueiros, coleiras, aparelhos hospitalares, ele...  2016-11-09    mercado   \n",
      "4  Toda vez que o grupo de amigos de Silvana Rabe...  2017-02-07  cotidiano   \n",
      "\n",
      "     subcategory                                               link  \n",
      "0      ruycastro  http://www1.folha.uol.com.br/colunas/ruycastro...  \n",
      "1  monicabergamo  http://www1.folha.uol.com.br/colunas/monicaber...  \n",
      "2            NaN  http://www1.folha.uol.com.br/esporte/2015/09/1...  \n",
      "3            NaN  http://www1.folha.uol.com.br/mercado/2016/09/1...  \n",
      "4            NaN  http://www1.folha.uol.com.br/cotidiano/2017/07...  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('C:/Users/Usuario/Desktop/word_embeeding_data/treino.csv')\n",
    "test_df = pd.read_csv('C:/Users/Usuario/Desktop/word_embeeding_data/teste.csv')\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACY\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = 'Rio de Janeiro'\n",
    "# doc = nlp(txt)\n",
    "# print(doc[2])\n",
    "# print(doc[1].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRATAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_treatment(nlp, data, labels = None):\n",
    "    tokens_validos = []\n",
    "    labels_list = []\n",
    "    step = max(1, len(data) // 50)  # Garante que step nunca seja 0\n",
    "    for i, doc in enumerate(nlp.pipe(data, batch_size=1000, n_process=-1)):\n",
    "        if i % step == 0:\n",
    "            print(f\"data {i}/{len(data)}\")\n",
    "        tokens = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "        if len(tokens) > 2:  # Adiciona apenas se houver mais de 2 palavras\n",
    "            tokens_validos.append(' '.join(tokens))\n",
    "            if labels is not None: labels_list.append(labels[i])\n",
    "\n",
    "    return tokens_validos, labels_list\n",
    "\n",
    "# treat,_ = data_treatment(nlp, ['Rio de 232 Janeiro é uma cidade','cidade cidade cidade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0/90000\n",
      "data 1800/90000\n",
      "data 3600/90000\n",
      "data 5400/90000\n",
      "data 7200/90000\n",
      "data 9000/90000\n",
      "data 10800/90000\n",
      "data 12600/90000\n",
      "data 14400/90000\n",
      "data 16200/90000\n",
      "data 18000/90000\n",
      "data 19800/90000\n",
      "data 21600/90000\n",
      "data 23400/90000\n",
      "data 25200/90000\n",
      "data 27000/90000\n",
      "data 28800/90000\n",
      "data 30600/90000\n",
      "data 32400/90000\n",
      "data 34200/90000\n",
      "data 36000/90000\n",
      "data 37800/90000\n",
      "data 39600/90000\n",
      "data 41400/90000\n",
      "data 43200/90000\n",
      "data 45000/90000\n",
      "data 46800/90000\n",
      "data 48600/90000\n",
      "data 50400/90000\n",
      "data 52200/90000\n",
      "data 54000/90000\n",
      "data 55800/90000\n",
      "data 57600/90000\n",
      "data 59400/90000\n",
      "data 61200/90000\n",
      "data 63000/90000\n",
      "data 64800/90000\n",
      "data 66600/90000\n",
      "data 68400/90000\n",
      "data 70200/90000\n",
      "data 72000/90000\n",
      "data 73800/90000\n",
      "data 75600/90000\n",
      "data 77400/90000\n",
      "data 79200/90000\n",
      "data 81000/90000\n",
      "data 82800/90000\n",
      "data 84600/90000\n",
      "data 86400/90000\n",
      "data 88200/90000\n",
      "                                               title   category\n",
      "0  polêmica marine le pen abomina negacionistas h...      mundo\n",
      "1  macron le pen turno frança revés siglas tradic...      mundo\n",
      "2  apesar larga vitória legislativas macron terá ...      mundo\n",
      "3  governo antecipa balanço alckmin anuncia queda...  cotidiano\n",
      "4       queda maio atividade econômica sobe junho bc    mercado\n"
     ]
    }
   ],
   "source": [
    "train_df_treat=pd.DataFrame()\n",
    "\n",
    "train_df_treat['title'], train_df_treat['category'] = data_treatment(nlp, train_df['title'], train_df['category'])\n",
    "\n",
    "print(train_df_treat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0/20513\n",
      "data 410/20513\n",
      "data 820/20513\n",
      "data 1230/20513\n",
      "data 1640/20513\n",
      "data 2050/20513\n",
      "data 2460/20513\n",
      "data 2870/20513\n",
      "data 3280/20513\n",
      "data 3690/20513\n",
      "data 4100/20513\n",
      "data 4510/20513\n",
      "data 4920/20513\n",
      "data 5330/20513\n",
      "data 5740/20513\n",
      "data 6150/20513\n",
      "data 6560/20513\n",
      "data 6970/20513\n",
      "data 7380/20513\n",
      "data 7790/20513\n",
      "data 8200/20513\n",
      "data 8610/20513\n",
      "data 9020/20513\n",
      "data 9430/20513\n",
      "data 9840/20513\n",
      "data 10250/20513\n",
      "data 10660/20513\n",
      "data 11070/20513\n",
      "data 11480/20513\n",
      "data 11890/20513\n",
      "data 12300/20513\n",
      "data 12710/20513\n",
      "data 13120/20513\n",
      "data 13530/20513\n",
      "data 13940/20513\n",
      "data 14350/20513\n",
      "data 14760/20513\n",
      "data 15170/20513\n",
      "data 15580/20513\n",
      "data 15990/20513\n",
      "data 16400/20513\n",
      "data 16810/20513\n",
      "data 17220/20513\n",
      "data 17630/20513\n",
      "data 18040/20513\n",
      "data 18450/20513\n",
      "data 18860/20513\n",
      "data 19270/20513\n",
      "data 19680/20513\n",
      "data 20090/20513\n",
      "data 20500/20513\n",
      "                                               title   category\n",
      "0  haddad congela orçamento suspende emendas vere...    colunas\n",
      "1  proposta reforma fifa divulgação salário cartolas    esporte\n",
      "2  mercado incipiente internet coisas conecta bue...    mercado\n",
      "3       mortes psicanalista estudou autismo crianças  cotidiano\n",
      "4                    pra entender estupidez maradona    esporte\n"
     ]
    }
   ],
   "source": [
    "test_df_treat=pd.DataFrame()\n",
    "\n",
    "test_df_treat['title'], test_df_treat['category'] = data_treatment(nlp, test_df['title'], test_df['category'])\n",
    "\n",
    "print(test_df_treat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 18:23:32,078 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2025-01-04T18:23:32.077543', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "2025-01-04 18:23:32,757 : - collecting all words and their counts\n",
      "2025-01-04 18:23:32,758 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-01-04 18:23:32,796 : - PROGRESS: at sentence #10000, processed 63841 words, keeping 14987 word types\n",
      "2025-01-04 18:23:32,826 : - PROGRESS: at sentence #20000, processed 127676 words, keeping 21031 word types\n",
      "2025-01-04 18:23:32,865 : - PROGRESS: at sentence #30000, processed 191537 words, keeping 25492 word types\n",
      "2025-01-04 18:23:32,897 : - PROGRESS: at sentence #40000, processed 255264 words, keeping 29051 word types\n",
      "2025-01-04 18:23:32,934 : - PROGRESS: at sentence #50000, processed 319240 words, keeping 31962 word types\n",
      "2025-01-04 18:23:32,970 : - PROGRESS: at sentence #60000, processed 383548 words, keeping 34516 word types\n",
      "2025-01-04 18:23:33,007 : - PROGRESS: at sentence #70000, processed 447605 words, keeping 36716 word types\n",
      "2025-01-04 18:23:33,043 : - PROGRESS: at sentence #80000, processed 511600 words, keeping 38810 word types\n",
      "2025-01-04 18:23:33,059 : - collected 39689 word types from a corpus of 540186 raw words and 84465 sentences\n",
      "2025-01-04 18:23:33,061 : - Creating a fresh vocabulary\n",
      "2025-01-04 18:23:33,194 : - Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 18184 unique words (45.82% of original 39689, drops 21505)', 'datetime': '2025-01-04T18:23:33.194517', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-04 18:23:33,196 : - Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 513044 word corpus (94.98% of original 540186, drops 27142)', 'datetime': '2025-01-04T18:23:33.195517', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-04 18:23:33,391 : - deleting the raw counts dictionary of 39689 items\n",
      "2025-01-04 18:23:33,394 : - sample=0.001 downsamples 8 most-common words\n",
      "2025-01-04 18:23:33,395 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 504278.90908523684 word corpus (98.3%% of prior 513044)', 'datetime': '2025-01-04T18:23:33.395359', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-04 18:23:33,665 : - estimated required memory for 18184 words and 300 dimensions: 52733600 bytes\n",
      "2025-01-04 18:23:33,666 : - resetting layer weights\n",
      "2025-01-04 18:23:33,718 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-04T18:23:33.718494', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "sg=0 -> cbow\n",
    "sg=1 -> skipgram\n",
    "\n",
    "window -> contexto das palavras(qtd antes e depois)\n",
    "\n",
    "vector_size -> tamanho da matriz\n",
    "\n",
    "min_count -> evita erros de digitação com um minimo necessário de repetição\n",
    "\n",
    "alpha -> learning rate\n",
    "min_alpha -> suavização do lr\n",
    "'''\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss após a época {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss após a época {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "\n",
    "model = Word2Vec(\n",
    "    sg = 0,\n",
    "    window=1,\n",
    "    vector_size=300,\n",
    "    min_count=3,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.007,\n",
    "    compute_loss = True,\n",
    "    callbacks=[callback()]\n",
    "    )\n",
    "\n",
    "token_list = [txt.split(' ') for txt in train_df_treat['title'].drop_duplicates().dropna()]\n",
    "model.build_vocab(token_list, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 18:23:33,754 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 18184 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=1 shrink_windows=True', 'datetime': '2025-01-04T18:23:33.754963', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2025-01-04 18:23:34,852 : - EPOCH 0 - PROGRESS: at 64.84% examples, 309171 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:35,467 : - EPOCH 0: training on 540186 raw words (504151 effective words) took 1.7s, 301665 effective words/s\n",
      "2025-01-04 18:23:36,530 : - EPOCH 1 - PROGRESS: at 62.99% examples, 304050 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:37,040 : - EPOCH 1: training on 540186 raw words (504359 effective words) took 1.6s, 324804 effective words/s\n",
      "2025-01-04 18:23:38,135 : - EPOCH 2 - PROGRESS: at 72.21% examples, 354171 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:38,578 : - EPOCH 2: training on 540186 raw words (504276 effective words) took 1.5s, 343111 effective words/s\n",
      "2025-01-04 18:23:39,634 : - EPOCH 3 - PROGRESS: at 70.36% examples, 342225 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:40,175 : - EPOCH 3: training on 540186 raw words (504450 effective words) took 1.6s, 319644 effective words/s\n",
      "2025-01-04 18:23:41,235 : - EPOCH 4 - PROGRESS: at 55.61% examples, 274990 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:41,840 : - EPOCH 4: training on 540186 raw words (504198 effective words) took 1.6s, 310835 effective words/s\n",
      "2025-01-04 18:23:42,886 : - EPOCH 5 - PROGRESS: at 66.69% examples, 334441 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:43,516 : - EPOCH 5: training on 540186 raw words (504252 effective words) took 1.6s, 308625 effective words/s\n",
      "2025-01-04 18:23:44,557 : - EPOCH 6 - PROGRESS: at 62.99% examples, 310386 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:45,127 : - EPOCH 6: training on 540186 raw words (504259 effective words) took 1.6s, 316698 effective words/s\n",
      "2025-01-04 18:23:46,150 : - EPOCH 7 - PROGRESS: at 64.84% examples, 323966 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:46,742 : - EPOCH 7: training on 540186 raw words (504321 effective words) took 1.6s, 315203 effective words/s\n",
      "2025-01-04 18:23:47,818 : - EPOCH 8 - PROGRESS: at 61.16% examples, 304869 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:48,345 : - EPOCH 8: training on 540186 raw words (504258 effective words) took 1.5s, 327926 effective words/s\n",
      "2025-01-04 18:23:49,393 : - EPOCH 9 - PROGRESS: at 74.07% examples, 362747 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:49,743 : - EPOCH 9: training on 540186 raw words (504113 effective words) took 1.4s, 365755 effective words/s\n",
      "2025-01-04 18:23:50,766 : - EPOCH 10 - PROGRESS: at 53.76% examples, 269576 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:51,540 : - EPOCH 10: training on 540186 raw words (504273 effective words) took 1.8s, 283557 effective words/s\n",
      "2025-01-04 18:23:52,664 : - EPOCH 11 - PROGRESS: at 64.85% examples, 303531 words/s, in_qsize 5, out_qsize 2\n",
      "2025-01-04 18:23:53,225 : - EPOCH 11: training on 540186 raw words (504125 effective words) took 1.6s, 308127 effective words/s\n",
      "2025-01-04 18:23:54,321 : - EPOCH 12 - PROGRESS: at 33.37% examples, 156487 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:23:55,361 : - EPOCH 12 - PROGRESS: at 77.76% examples, 185487 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:23:55,902 : - EPOCH 12: training on 540186 raw words (504329 effective words) took 2.7s, 189991 effective words/s\n",
      "2025-01-04 18:23:57,137 : - EPOCH 13 - PROGRESS: at 44.51% examples, 203171 words/s, in_qsize 6, out_qsize 1\n",
      "2025-01-04 18:23:58,162 : - EPOCH 13 - PROGRESS: at 79.60% examples, 188690 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:23:58,832 : - EPOCH 13: training on 540186 raw words (504345 effective words) took 2.8s, 180235 effective words/s\n",
      "2025-01-04 18:23:59,919 : - EPOCH 14 - PROGRESS: at 33.37% examples, 159616 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:00,925 : - EPOCH 14 - PROGRESS: at 83.30% examples, 203994 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:24:01,214 : - EPOCH 14: training on 540186 raw words (504269 effective words) took 2.3s, 214908 effective words/s\n",
      "2025-01-04 18:24:02,291 : - EPOCH 15 - PROGRESS: at 51.92% examples, 253428 words/s, in_qsize 6, out_qsize 1\n",
      "2025-01-04 18:24:03,082 : - EPOCH 15: training on 540186 raw words (504194 effective words) took 1.8s, 276554 effective words/s\n",
      "2025-01-04 18:24:04,190 : - EPOCH 16 - PROGRESS: at 55.61% examples, 272432 words/s, in_qsize 4, out_qsize 1\n",
      "2025-01-04 18:24:04,994 : - EPOCH 16: training on 540186 raw words (504312 effective words) took 1.8s, 275173 effective words/s\n",
      "2025-01-04 18:24:06,112 : - EPOCH 17 - PROGRESS: at 64.84% examples, 310542 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:06,688 : - EPOCH 17: training on 540186 raw words (504295 effective words) took 1.6s, 309809 effective words/s\n",
      "2025-01-04 18:24:07,749 : - EPOCH 18 - PROGRESS: at 66.69% examples, 334265 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:08,295 : - EPOCH 18: training on 540186 raw words (504260 effective words) took 1.6s, 325327 effective words/s\n",
      "2025-01-04 18:24:09,315 : - EPOCH 19 - PROGRESS: at 64.84% examples, 326064 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:09,919 : - EPOCH 19: training on 540186 raw words (504222 effective words) took 1.6s, 313933 effective words/s\n",
      "2025-01-04 18:24:10,997 : - EPOCH 20 - PROGRESS: at 62.99% examples, 302884 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:11,621 : - EPOCH 20: training on 540186 raw words (504321 effective words) took 1.7s, 301651 effective words/s\n",
      "2025-01-04 18:24:12,695 : - EPOCH 21 - PROGRESS: at 62.99% examples, 312491 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:24:13,302 : - EPOCH 21: training on 540186 raw words (504130 effective words) took 1.6s, 311010 effective words/s\n",
      "2025-01-04 18:24:14,391 : - EPOCH 22 - PROGRESS: at 68.54% examples, 330025 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:14,888 : - EPOCH 22: training on 540186 raw words (504189 effective words) took 1.5s, 326605 effective words/s\n",
      "2025-01-04 18:24:15,940 : - EPOCH 23 - PROGRESS: at 63.01% examples, 309593 words/s, in_qsize 6, out_qsize 2\n",
      "2025-01-04 18:24:16,454 : - EPOCH 23: training on 540186 raw words (504276 effective words) took 1.5s, 327523 effective words/s\n",
      "2025-01-04 18:24:17,530 : - EPOCH 24 - PROGRESS: at 64.85% examples, 309694 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:18,060 : - EPOCH 24: training on 540186 raw words (504239 effective words) took 1.6s, 318232 effective words/s\n",
      "2025-01-04 18:24:19,138 : - EPOCH 25 - PROGRESS: at 68.54% examples, 342478 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:19,626 : - EPOCH 25: training on 540186 raw words (504236 effective words) took 1.5s, 337177 effective words/s\n",
      "2025-01-04 18:24:20,739 : - EPOCH 26 - PROGRESS: at 68.54% examples, 326901 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:24:21,285 : - EPOCH 26: training on 540186 raw words (504341 effective words) took 1.6s, 314705 effective words/s\n",
      "2025-01-04 18:24:22,378 : - EPOCH 27 - PROGRESS: at 66.69% examples, 331147 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:24:22,876 : - EPOCH 27: training on 540186 raw words (504295 effective words) took 1.5s, 333400 effective words/s\n",
      "2025-01-04 18:24:23,914 : - EPOCH 28 - PROGRESS: at 66.69% examples, 330132 words/s, in_qsize 5, out_qsize 0\n",
      "2025-01-04 18:24:24,433 : - EPOCH 28: training on 540186 raw words (504349 effective words) took 1.5s, 328062 effective words/s\n",
      "2025-01-04 18:24:25,458 : - EPOCH 29 - PROGRESS: at 64.84% examples, 324917 words/s, in_qsize 6, out_qsize 0\n",
      "2025-01-04 18:24:26,034 : - EPOCH 29: training on 540186 raw words (504333 effective words) took 1.6s, 318848 effective words/s\n",
      "2025-01-04 18:24:26,038 : - Word2Vec lifecycle event {'msg': 'training on 16205580 raw words (15127970 effective words) took 52.3s, 289371 effective words/s', 'datetime': '2025-01-04T18:24:26.037487', 'gensim': '4.3.3', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15127970, 16205580)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(\n",
    "    token_list, \n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walmart', 0.5452494025230408),\n",
       " ('tesla', 0.506166934967041),\n",
       " ('autópsia', 0.47925975918769836),\n",
       " ('folhainvest', 0.4775085151195526),\n",
       " ('amazon', 0.4739232063293457),\n",
       " ('toyota', 0.46855252981185913),\n",
       " ('samsung', 0.46721357107162476),\n",
       " ('sony', 0.4558829963207245),\n",
       " ('sabmiller', 0.452098548412323),\n",
       " ('fbi', 0.44993138313293457)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 18:41:57,640 : - storing 18184x300 projection weights into model/cbow_model.txt\n"
     ]
    }
   ],
   "source": [
    "model.wv.save_word2vec_format('model/cbow_model.txt', binary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
