{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDENTIFICADOR DE IDIOMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://regex101.com/\n",
    "\n",
    "() - cria grupos de palavras (a|b)\n",
    "[] - cria grupos de caracteres a[abc]c\n",
    "- - intervalo 1-5\n",
    "| - ou\n",
    "* - 0 ou n vezes a repeticao [a-z]*\n",
    "+ - n vezes a repeticao [a-z]+\n",
    "{} - intervalo de n vezes de repeticao [a-z]{1,5}\n",
    "\n",
    "^ - inicio da string ^g.* vai detectar tudo que começa com g\n",
    "\n",
    "$ - fim da string $g.* vai detectar tudo que termina com g\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEGAR OS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.util import bigrams, everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.lm import MLE, NgramCounter, Laplace\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pt = pd.read_csv(\"data/stackoverflow_portugues.csv\")\n",
    "data_in = pd.read_csv(\"data/stackoverflow_ingles.csv\")\n",
    "data_es = pd.read_csv(\"data/stackoverflow_espanhol.csv\" , delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRATAR OS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substituir_regex(data, regex, substituir = ''):\n",
    "    if type(data) == str:\n",
    "        return regex.sub(substituir,data)\n",
    "    return [regex.sub(substituir, dat) for dat in data]\n",
    "\n",
    "def regex_treatment(data):    \n",
    "    regex_code = re.compile(r'<code>.*?</code>') # Remove partes com código\n",
    "    regex_html = re.compile(r'<(.|(\\n))*?>') # Remove as tags HTML\n",
    "    regex_punctuation = re.compile(r'[^\\w\\s]') # Remove as pontuações\n",
    "    regex_digit = re.compile(r'\\d+') # Remove dígitos\n",
    "    regex_line = re.compile(r'\\n') # Remove quebra de linha\n",
    "\n",
    "    # Realiza substituições sequenciais\n",
    "    data = substituir_regex(data, regex_code, \"---CODE---\")\n",
    "    data = substituir_regex(data, regex_html)\n",
    "    data = substituir_regex(data, regex_punctuation)\n",
    "    data = substituir_regex(data, regex_digit)\n",
    "    data = substituir_regex(data, regex_line, ' ')\n",
    "\n",
    "    # Remove espaços duplicados ao final\n",
    "    regex_duplicate_space = re.compile(r'\\s+')  # Normaliza múltiplos espaços\n",
    "    data = substituir_regex(data, regex_duplicate_space, ' ')\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    data = [txt.lower().strip() for txt in data]  # Remove espaços extras nas bordas\n",
    "\n",
    "    return data\n",
    "\n",
    "data_pt['regex_quest'] = regex_treatment(data_pt['Questão'])\n",
    "data_es['regex_quest'] = regex_treatment(data_es['Questão'])\n",
    "data_in['regex_quest'] = regex_treatment(data_in['Questão'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se eu fizer o hash de senhas antes de armazenálas em meu banco de dados é suficiente para evitar que elas sejam recuperadas por alguém estou falando apenas da recuperação diretamente do banco de dados e não qualquer outro tipo de ataque como força bruta na página de login da aplicação keylogger no cliente e criptoanálise rubberhose qualquer forma de hash não vai impedir esses ataques tenho preocupação em dificultar ou até impossibilitar a obtenção das senhas originais caso o banco de dados seja comprometido como dar maior garantia de segurança neste aspecto quais preocupações adicionais evitariam o acesso às senhas existem formas melhores de fazer esse hash\n",
      "las sentencias dinámicas son sentencias sql que se crean como cadenas de texto strings y en las que se insertanconcatenan valores obtenidos de alguna fuente normalmente proveniente del usuario lo que puede hacer que sean vulnerables a inyección sql si no se sanean las entradas como por ejemplo id_usuario _postid mysql_queryselect from usuarios where id id_usuario eso es un ejemplo de una vulnerabilidad grave en la seguridad de una aplicación web o no porque si el usuario introdujese un valor como code nos encontraríamos con que la sentencia ejecutada sería select from usuarios where id drop table usuarios y se eliminaría la tabla usuarios con todos los datos contenidos en ella cómo puedo evitar que la inyección sql ocurra en php\n",
      "here is a piece of c code that seems very peculiar for some strange reason sorting the data miraculously makes the code almost six times faster include ltalgorithmgt include ltctimegt include ltiostreamgt int main generate data const unsigned arraysize int dataarraysize for unsigned c c lt arraysize c datac stdrand with this the next loop runs faster stdsortdata data arraysize test clock_t start clock long long sum for unsigned i i lt i primary loop for unsigned c c lt arraysize c if datac gt sum datac double elapsedtime static_castltdoublegtclock start clocks_per_sec stdcout ltlt elapsedtime ltlt stdendl stdcout ltlt sum ltlt sum ltlt stdendl without code the code runs in seconds with the sorted data the code runs in seconds initially i thought this might be just a language or compiler anomaly so i tried it in java import javautilarrays import javautilrandom public class main public static void mainstring args generate data int arraysize int data new intarraysize random rnd new random for int c c lt arraysize c datac rndnextint with this the next loop runs faster arrayssortdata test long start systemnanotime long sum for int i i lt i primary loop for int c c lt arraysize c if datac gt sum datac systemoutprintlnsystemnanotime start systemoutprintlnsum sum with a somewhat similar but less extreme result my first thought was that sorting brings the data into the cache but then i thought how silly that is because the array was just generated what is going on why is it faster to process a sorted array than an unsorted array the code is summing up some independent terms and the order should not matter\n"
     ]
    }
   ],
   "source": [
    "print(data_pt['regex_quest'][0])\n",
    "print(data_es['regex_quest'][0])\n",
    "print(data_in['regex_quest'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(data, ngram = 2):\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    return padded_everygram_pipeline(ngram, tokenizer.tokenize(data))\n",
    "\n",
    "def treinar_mle(txt, nmgram = 2):\n",
    "    all_words, vocabulario = word_tokenize(' '.join(txt))\n",
    "\n",
    "    model = MLE(nmgram)\n",
    "    model.fit(all_words, vocabulario)\n",
    "    return model\n",
    "\n",
    "def treinar_laplace(txt, nmgram = 2):\n",
    "    all_words, vocabulario = word_tokenize(' '.join(txt))\n",
    "\n",
    "    model = Laplace(nmgram)\n",
    "    model.fit(all_words, vocabulario)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 13\n",
    "test_size = 0.1\n",
    "\n",
    "pt_train, pt_test = train_test_split(data_pt['regex_quest'], random_state = random_seed, test_size=test_size)\n",
    "es_train, es_test = train_test_split(data_es['regex_quest'], random_state = random_seed, test_size=test_size)\n",
    "in_train, in_test = train_test_split(data_in['regex_quest'], random_state = random_seed, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt = treinar_laplace(pt_train)\n",
    "model_es = treinar_laplace(es_train)\n",
    "model_in = treinar_laplace(in_train)\n",
    "\n",
    "models = [\n",
    "    model_pt,\n",
    "    model_es,\n",
    "    model_in,\n",
    "]\n",
    "\n",
    "model_labels = {\n",
    "    0: 'Português',\n",
    "    1: 'Espanhol',\n",
    "    2: 'Inglês',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_data(data):\n",
    "    regex_punctuation = re.compile(r'[^\\w\\s]') # Remove as pontuações\n",
    "    regex_digit = re.compile(r'\\d+') # Remove dígitos\n",
    "    regex_line = re.compile(r'\\n') # Remove quebra de linha\n",
    "\n",
    "    # Realiza substituições sequenciais\n",
    "    data = substituir_regex(data, regex_punctuation)\n",
    "    data = substituir_regex(data, regex_digit)\n",
    "    data = substituir_regex(data, regex_line, ' ')\n",
    "\n",
    "    # Remove espaços duplicados ao final\n",
    "    regex_duplicate_space = re.compile(r'\\s+')  # Normaliza múltiplos espaços\n",
    "    data = substituir_regex(data, regex_duplicate_space, ' ')\n",
    "    return data\n",
    "\n",
    "def get_bigrams(data):\n",
    "    data = regex_data(data)\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(data.lower())\n",
    "\n",
    "    fakechar = [list(pad_both_ends(palavra, n=2)) for palavra in tokens]\n",
    "    return [list(bigrams(palavra)) for palavra in fakechar]\n",
    "\n",
    "def test_perplexity(texto, model):\n",
    "    return sum([model.perplexity(txt) for txt in get_bigrams(texto)])\n",
    "\n",
    "# test_perplexity(pt_test.iloc[0], model_in)\n",
    "\n",
    "def get_best_perplexity(texto, models):\n",
    "    perplexity_list = [(test_perplexity(texto, model)) for model in models]\n",
    "    best_index = perplexity_list.index(min(perplexity_list))\n",
    "    return perplexity_list, best_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.302354839529755, 124.9748613770786, 125.79691568632538]\n",
      "Português\n"
     ]
    }
   ],
   "source": [
    "perplexity_list, best_index = get_best_perplexity('te amo meu mozão', models)\n",
    "print(perplexity_list)\n",
    "print(model_labels[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(results, val):\n",
    "    return results.count(val)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "resultados_pt = []\n",
    "for result in pt_test:\n",
    "    perplexity_list, best_index = get_best_perplexity(result, models)\n",
    "    resultados_pt.append(best_index)\n",
    "\n",
    "print(get_accuracy(resultados_pt,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "resultados_es = []\n",
    "for result in es_test:\n",
    "    perplexity_list, best_index = get_best_perplexity(result, models)\n",
    "    resultados_es.append(best_index)\n",
    "\n",
    "print(get_accuracy(resultados_es,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "resultados_in = []\n",
    "for result in in_test:\n",
    "    perplexity_list, best_index = get_best_perplexity(result, models)\n",
    "    resultados_in.append(best_index)\n",
    "\n",
    "print(get_accuracy(resultados_in, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/models.pkl\", 'wb') as arquivo:\n",
    "    pickle.dump(models, arquivo)\n",
    "\n",
    "with open(\"model/model_labels.pkl\", 'wb') as arquivo:\n",
    "    pickle.dump(model_labels, arquivo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
